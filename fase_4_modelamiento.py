# -*- coding: utf-8 -*-
"""Fase_4_Modelamiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zrxh2uuc5Ea6Ue6UMzDHy5M-Pe1VuHj6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from scipy import stats
import scipy.stats as stats
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV
import warnings
warnings.filterwarnings('ignore')

#cargamos los datos
df = pd.read_csv('df_limpio_modelo.csv')
df.head()

print(df['target'].value_counts())

#grafica de target
sns.countplot(x='target', data=df)
plt.show()

#separamos las caracteristicas y variable objetivo
X = df.drop('target', axis=1)
y = df['target']

#Dividimos conjunto 70% y 30 %
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

#aplciamos balanceo de datos al 70%
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_temp, y_temp)

print("DistribuciÃ³n antes del balanceo:")
print(y_temp.value_counts())
print("\nDistribuciÃ³n despuÃ©s del balanceo (SMOTE):")
print(y_train.value_counts())
print(f"\nTamaÃ±o del conjunto de entrenamiento: {X_train.shape}")
print(f"TamaÃ±o del conjunto de prueba: {X_test.shape}")

#estandarizcion
scaler = StandardScaler()
X_train_sacled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#grafica del balanceo de datos
sns.countplot(x=y_train)
plt.show()

#validacion cruzada
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

resultados = {}
model_performance = {}

"""MODELO 1

REDES NEURONALES
"""

# Modelo Red Neuronal
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(100, 50),
                    activation = 'relu', solver = 'adam',
                    alpha= 0.001, batch_size ='auto', learning_rate = 'constant',
                    learning_rate_init = 0.001, max_iter = 1000,random_state = 42,
                    early_stopping = True, validation_fraction = 0.2, n_iter_no_change=10
                    )
#validacion cruzada
nn_scores = cross_val_score(mlp, X_train_sacled, y_train, cv=cv, scoring='accuracy')
nn_cv_precision = cross_val_score(mlp, X_train_sacled, y_train, cv=cv, scoring='precision')
nn_cv_recall = cross_val_score(mlp, X_train_sacled, y_train, cv=cv, scoring='recall')
nn_cv_f1 = cross_val_score(mlp, X_train_sacled, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {nn_scores.mean():.4f} (+/- {nn_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {nn_cv_precision.mean():.4f} (+/- {nn_cv_precision.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {nn_cv_recall.mean():.4f} (+/- {nn_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {nn_cv_f1.mean():.4f} (+/- {nn_cv_f1.std() * 2:.4f})")

#evaluacion en test
mlp.fit(X_train_sacled, y_train)
y_pred_nn = mlp.predict(X_test_scaled)
y_pred_proba_nn = mlp.predict_proba(X_test_scaled)[:, 1]

#METRICAS
accuracy_nn = accuracy_score(y_test, y_pred_nn)
precision_nn = precision_score(y_test, y_pred_nn)
recall_nn = recall_score(y_test, y_pred_nn)
f1_nn = f1_score(y_test, y_pred_nn)
roc_auc_nn = roc_auc_score(y_test, y_pred_proba_nn)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_nn:.4f}")
print(f"Precision: {precision_nn:.4f}")
print(f"Recall: {recall_nn:.4f}")
print(f"F1-Score: {f1_nn:.4f}")
print(f"ROC AUC: {roc_auc_nn:.4f}")

print(f"numero de capas : {mlp.n_layers_}")
print(f"numero de iteraciones : {mlp.n_iter_}")
print(f"perdida final : {mlp.loss_:4f}")

# Guardar resultados
resultados['Red Neuronal'] = {
    'Accuracy': accuracy_nn,
    'Precision': precision_nn,
    'Recall': recall_nn,
    'F1-Score': f1_nn,
    'ROC AUC': roc_auc_nn
}

model_performance['Neural Network'] = accuracy_nn

"""Modelo 2
SVM
"""

#modelo SVM
svm_model = SVC(probability=True, random_state=42)

#validacion cruzada
svm_scores = cross_val_score(svm_model, X_train_sacled, y_train, cv=cv, scoring='accuracy')
svm_cv_precision = cross_val_score(svm_model, X_train_sacled, y_train, cv=cv, scoring='precision')
svm_cv_recall = cross_val_score(svm_model, X_train_sacled, y_train, cv=cv, scoring='recall')
svm_cv_f1 = cross_val_score(svm_model, X_train_sacled, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {svm_scores.mean():.4f} (+/- {svm_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {svm_cv_precision.mean():.4f} (+/- {svm_cv_precision.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {svm_cv_recall.mean():.4f} (+/- {svm_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {svm_cv_f1.mean():.4f} (+/- {svm_cv_f1.std() * 2:.4f})")

#evaluacion
svm_model.fit(X_train_sacled, y_train)
y_pred_svm = svm_model.predict(X_test_scaled)
y_pred_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1]

#Metricas
accuracy_svm = accuracy_score(y_test, y_pred_svm)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
roc_auc_svm = roc_auc_score(y_test, y_pred_proba_svm)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_svm:.4f}")
print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-Score: {f1_svm:.4f}")
print(f"ROC AUC: {roc_auc_svm:.4f}")

#guardar resultados
resultados['SVM'] = {
    'CV Accuracy': svm_scores.mean(),
    'CV Precision': svm_cv_precision.mean(),
    'CV Recall': svm_cv_recall.mean(),
    'CV F1-Score': svm_cv_f1.mean(),
    'Accuracy': accuracy_svm,
    'Precision': precision_svm,
    'Recall': recall_svm,
    'F1-Score': f1_svm,
    'ROC AUC': roc_auc_svm,
    'cv_scores': svm_scores.tolist()
}

model_performance['SVM'] = accuracy_svm

"""MODELO 3
KNN
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
#modelo KNN
knn_model = KNeighborsClassifier()

#Validacion Cruzada
knn_scores = cross_val_score(knn_model, X_train_sacled, y_train, cv=cv, scoring='accuracy')
knn_cv_precision = cross_val_score(knn_model, X_train_sacled, y_train, cv=cv, scoring='precision')
knn_cv_recall = cross_val_score(knn_model, X_train_sacled, y_train, cv=cv, scoring='recall')
knn_cv_f1 = cross_val_score(knn_model, X_train_sacled, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {knn_scores.mean():.4f} (+/- {knn_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {knn_cv_precision.mean():.4f} (+/- {knn_cv_precision.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {knn_cv_recall.mean():.4f} (+/- {knn_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {knn_cv_f1.mean():.4f} (+/- {knn_cv_f1.std() * 2:.4f})")

#evaluacion
knn_model.fit(X_train_sacled, y_train)
y_pred_knn = knn_model.predict(X_test_scaled)
y_pred_proba_knn = knn_model.predict_proba(X_test_scaled)[:, 1]

#Metricas
accuracy_knn = accuracy_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
roc_auc_knn = roc_auc_score(y_test, y_pred_proba_knn)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_knn:.4f}")
print(f"Precision: {precision_knn:.4f}")
print(f"Recall: {recall_knn:.4f}")
print(f"F1-Score: {f1_knn:.4f}")
print(f"ROC AUC: {roc_auc_knn:.4f}")

#guardar resultados
resultados['KNN'] = {
    'CV Accuracy': knn_scores.mean(),
    'CV Precision': knn_cv_precision.mean(),
    'CV Recall': knn_cv_recall.mean(),
    'CV F1-Score': knn_cv_f1.mean(),
    'Accuracy': accuracy_knn,
    'Precision': precision_knn,
    'Recall': recall_knn,
    'F1-Score': f1_knn,
    'ROC AUC': roc_auc_knn,
    'cv_scores': knn_scores.tolist()
}

model_performance['KNN'] = precision_knn

"""MODELO 4
ARBOL DE DECISION
"""

#modelo Arbol de decicion
dt_model = DecisionTreeClassifier(random_state=42)

#validacion cruzada
dt_scores = cross_val_score(dt_model, X_train, y_train, cv=cv, scoring='accuracy')
dt_cv_precision = cross_val_score(dt_model, X_train, y_train, cv=cv, scoring='precision')
dt_cv_recall = cross_val_score(dt_model, X_train, y_train, cv=cv, scoring='recall')
dt_cv_f1 = cross_val_score(dt_model, X_train, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {dt_scores.mean():.4f} (+/- {dt_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {dt_cv_precision.mean():.4f} (+/- {dt_cv_precision.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {dt_cv_recall.mean():.4f} (+/- {dt_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {dt_cv_f1.mean():.4f} (+/- {dt_cv_f1.std() * 2:.4f})")

#evaluacion
dt_model.fit(X_train_sacled, y_train)
y_pred_dt = dt_model.predict(X_test)
y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]

#Metricas
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)
roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_dt:.4f}")
print(f"Precision: {precision_dt:.4f}")
print(f"Recall: {recall_dt:.4f}")
print(f"F1-Score: {f1_dt:.4f}")
print(f"ROC AUC: {roc_auc_dt:.4f}")

#guardar resultados
resultados['Arbol de Decision'] = {
    'CV Accuracy': dt_scores.mean(),
    'CV Precision': dt_cv_precision.mean(),
    'CV Recall': dt_cv_recall.mean(),
    'CV F1-Score': dt_cv_f1.mean(),
    'Accuracy': accuracy_dt,
    'Precision': precision_dt,
    'Recall': recall_dt,
    'F1-Score': f1_dt,
    'ROC AUC': roc_auc_dt,
    'cv_scores': dt_scores.tolist()
}

model_performance['Arbol de Decision'] = accuracy_dt

"""MODELO 5
RANDOM FOREST (ENSAMBLE)
"""

rf_model = RandomForestClassifier(random_state=42)

#Validacion Cruzada
rf_scores = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='accuracy')
rf_cv_precision = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='precision')
rf_cv_recall = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='recall')
rf_cv_f1 = cross_val_score(rf_model, X_train, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {rf_scores.mean():.4f} (+/- {rf_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {rf_cv_precision.mean():.4f} (+/- {rf_cv_precision.std() *2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {rf_cv_recall.mean():.4f} (+/- {rf_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {rf_cv_f1.mean():.4f} (+/- {rf_cv_f1.std() * 2:.4f})")

#evaluacion
rf_model.fit(X_train_sacled, y_train)
y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]

#metricas
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
roc_auc_rf = roc_auc_score(y_test, y_pred_proba_rf)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-Score: {f1_rf:.4f}")
print(f"ROC AUC: {roc_auc_rf:.4f}")

#guardo
resultados['Random Forest'] = {
    'CV Accuracy': rf_scores.mean(),
    'CV Precision': rf_cv_precision.mean(),
    'CV Recall': rf_cv_recall.mean(),
    'CV F1-Score': rf_cv_f1.mean(),
    'Accuracy': accuracy_rf,
    'Precision': precision_rf,
    'Recall': recall_rf,
    'F1-Score': f1_rf,
    'ROC AUC': roc_auc_rf,
    'cv_scores': rf_scores.tolist()
}

model_performance['Random Forest'] = accuracy_rf

"""MODELO 6 ADA BOOST

"""

from sklearn.ensemble import AdaBoostClassifier
ada_model = AdaBoostClassifier(n_estimators=100, random_state=42, learning_rate=0.1)

#validacion cruzada
ada_scores = cross_val_score(ada_model, X_train, y_train, cv=cv, scoring='accuracy')
ada_cv_precision = cross_val_score(ada_model, X_train, y_train, cv=cv, scoring='precision')
ada_cv_recall = cross_val_score(ada_model, X_train, y_train, cv=cv, scoring='recall')
ada_cv_f1 = cross_val_score(ada_model, X_train, y_train, cv=cv, scoring='f1')

print(f"ValidaciÃ³n Cruzada - Accuracy: {ada_scores.mean():.4f} (+/- {ada_scores.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Precision: {ada_cv_precision.mean():.4f} (+/- {ada_cv_precision.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - Recall: {ada_cv_recall.mean():.4f} (+/- {ada_cv_recall.std() * 2:.4f})")
print(f"ValidaciÃ³n Cruzada - F1-Score: {ada_cv_f1.mean():.4f} (+/- {ada_cv_f1.std() * 2:.4f})")

#evaluacion
ada_model.fit(X_train, y_train)
y_pred_ada = ada_model.predict(X_test)
y_pred_proba_ada = ada_model.predict_proba(X_test)[:,1]

#mtrsicas
accuracy_ada = accuracy_score(y_test, y_pred_ada)
precision_ada = precision_score(y_test, y_pred_ada)
recall_ada = recall_score(y_test, y_pred_ada)
f1_ada = f1_score(y_test, y_pred_ada)
roc_auc_ada = roc_auc_score(y_test, y_pred_proba_ada)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_ada:.4f}")
print(f"Precision: {precision_ada:.4f}")
print(f"Recall: {recall_ada:.4f}")
print(f"F1-Score: {f1_ada:.4f}")
print(f"ROC AUC: {roc_auc_ada:.4f}")

#importancias
feature_importance_ada = pd.DataFrame({'Feature': X.columns,
                                       'Importance': ada_model.feature_importances_
                                       }).sort_values(by='Importance', ascending=False)

print(f"Importancia de caracterÃ­sticas:\n{feature_importance_ada}")
print(feature_importance_ada.head(5))

#guardar cambios
resultados['AdaBoost'] = {
    'CV Accuracy': ada_scores.mean(),
    'CV Precision': ada_cv_precision.mean(),
    'CV Recall': ada_cv_recall.mean(),
    'CV F1-Score': ada_cv_f1.mean(),
    'Accuracy': accuracy_ada,
    'Precision': precision_ada,
    'Recall': recall_ada,
    'F1-Score': f1_ada,
    'ROC AUC': roc_auc_ada,
    'feature_importance': feature_importance_ada,
    'cv_scores': ada_scores.tolist()
}

model_performance['AdaBoost'] = accuracy_ada

"""MODELO 7: STACKING"""

from sklearn.ensemble import StackingClassifier

#Modelos base
base_estimators =[
    ('rf', RandomForestClassifier(n_estimators=50 ,random_state=42)),
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('knn', KNeighborsClassifier()),
    ('svm', SVC(probability=True, random_state=42)),
]

#Definicion
meta_estimator = LogisticRegression(random_state=42, max_iter=1000)

#Stacking
stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=meta_estimator, cv=5, passthrough=True)#No incluir originales


#validacion Cruzada
stacking_scores = cross_val_score(stacking_model, X_train, y_train, cv=cv, scoring='accuracy')

print(f"ValidaciÃ³n Cruzada - Accuracy: {stacking_scores.mean():.4f} (+/- {stacking_scores.std() * 2:.4f})")

#evalucaion
print("Entrenamiento del modelo Stacking...")
stacking_model.fit(X_train, y_train)
y_pred_stacking = stacking_model.predict(X_test)
y_pred_proba_stacking = stacking_model.predict_proba(X_test)[:, 1]

#metricas
accuracy_stacking = accuracy_score(y_test, y_pred_stacking)
precision_stacking = precision_score(y_test, y_pred_stacking)
recall_stacking = recall_score(y_test, y_pred_stacking)
f1_stacking = f1_score(y_test, y_pred_stacking)
roc_auc_stacking = roc_auc_score(y_test, y_pred_proba_stacking)

print("\nResultados en el conjunto de prueba:")
print(f"Accuracy: {accuracy_stacking:.4f}")
print(f"Precision: {precision_stacking:.4f}")
print(f"Recall: {recall_stacking:.4f}")
print(f"F1-Score: {f1_stacking:.4f}")
print(f"ROC AUC: {roc_auc_stacking:.4f}")

#Informacion
print("INFOMRACION DEL STACKING:")
print(f"modelos:{[name for name, _ in base_estimators]}")
print(f"modelo meta: {meta_estimator},__name__")

#Guardar
resultados['Stacking'] = {
    'CV Accuracy': stacking_scores.mean(),
    'Accuracy': accuracy_stacking,
    'Precision': precision_stacking,
    'Recall': recall_stacking,
    'F1-Score': f1_stacking,
    'ROC AUC': roc_auc_stacking,
    'cv_scores': stacking_scores.tolist()
}

model_performance['Stacking'] = accuracy_stacking

"""Analisis de resultados

"""

print("Resultados de los modelos:")

# CreaciÃ³n de dataframe
comparacion_df = pd.DataFrame({
    'Modelo': list(resultados.keys()),
    'Accuracy': [resultados[model].get('Accuracy', np.nan) for model in resultados],
    'Precision': [resultados[model].get('Precision', np.nan) for model in resultados],
    'Recall': [resultados[model].get('Recall', np.nan) for model in resultados],
    'F1-Score': [resultados[model].get('F1-Score', np.nan) for model in resultados],
    'ROC AUC': [resultados[model].get('ROC AUC', np.nan) for model in resultados],
})

# Ordenar por accuracy
comparacion_df = comparacion_df.sort_values(by='Accuracy', ascending=False)
print(comparacion_df)

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

#Accuracy y F-1
models = comparacion_df['Modelo']
x = np.arange(len(models))
width = 0.35

axes[0, 0].bar(x, comparacion_df['Accuracy'], width, label='Accuracy',alpha =0.7)
axes[0, 0].bar([i +width for i in x], width, label='F1-Score',alpha =0.7)
axes[0, 0].set_title('Comparacion Accuracy y F1-Score por modelo')
axes[0, 0].set_xlabel('Modelo')
axes[0, 0].set_ylabel('Puntos')
axes[0, 0].set_xticks([i + width/2 for i in x])
axes[0, 0].set_xticklabels(models, rotation=45)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Precision y recall
axes[0, 1].bar(x, comparacion_df['Precision'], width, label='Precision',alpha =0.7)
axes[0, 1].bar([i + width for i in x], comparacion_df['Recall'], width, label='Recall',alpha =0.7)
axes[0, 1].set_title('Comparacion Precision y Recall por modelo')
axes[0, 1].set_xlabel('Modelo')
axes[0, 1].set_ylabel('Puntos')
axes[0, 1].set_xticks([i + width/2 for i in x])
axes[0, 1].set_xticklabels(models, rotation=45)
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# ROC
axes[1, 0].bar(x, comparacion_df['ROC AUC'],alpha=0.7,color='purple')
axes[1, 0].set_title('Comparacion ROC por modelo')
axes[1, 0].set_xlabel('Modelo')
axes[1, 0].set_ylabel('AUC-ROC')
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].grid(True, alpha=0.3)

"""Seleccion del mejor modelo

"""

print("=" * 60)
print("SELECCIÃ“N DEL MEJOR MODELO")
print("=" * 60)

model_comparacion = []
for model_name, metrics in resultados.items():
  model_comparacion.append({
      'Modelo': model_name,
      'Accuracy': metrics.get('Accuracy', np.nan),
      'Precision': metrics.get('Precision', np.nan),
      'Recall': metrics.get('Recall', np.nan),
      'F1-Score': metrics.get('F1-Score', np.nan),
      'ROC AUC': metrics.get('ROC AUC', np.nan),
  })

model_comparacion_df = pd.DataFrame(model_comparacion)
model_comparacion_df = model_comparacion_df.sort_values(by='Accuracy', ascending=False)
print(model_comparacion_df.to_string(index=False))

#seleccionar los 3 mejores
mejores_modelos = model_comparacion_df.head(3)['Modelo'].tolist()
print(mejores_modelos)

from scipy.stats import f_oneway
#compararacion con Anova y Tukey

cv_scores_dict ={}
for model_name in mejores_modelos:
  cv_scores = resultados[model_name].get('cv_scores',[])
  if len(cv_scores) > 0:
    cv_scores_dict[model_name] = cv_scores
    print(f"{model_name}:{len(cv_scores)}")
  else:
    print(f"No se encontraron cv_scores para {model_name}")
if len(cv_scores_dict) >= 2:
  anova_data = []
  model_labels = []
  for model_name, scores in cv_scores_dict.items():
    anova_data.extend(scores)
    model_labels.extend([model_name] * len(scores))
#ANOVA
anova_df = pd.DataFrame({
    'Score': anova_data,
    'Modelo': model_labels
})
print(anova_df.groupby('Modelo')['Score'].describe())

#test
f_statistic, p_value = f_oneway(*[cv_scores_dict[model]
for model in cv_scores_dict.keys()])
print(f"F-statistic: {f_statistic:.4f}")
print(f"P-value: {p_value:-.4f}")
if p_value < 0.05:
  print("Hay diferencias significativas entre los modelos.")
else:
  print("No hay diferencias significativas entre los modelos.")

"""TUKEY

"""

# VersiÃ³n simplificada y mÃ¡s robusta para la interpretaciÃ³n de Tukey
print("\nInterpretaciÃ³n de Tukey HSD (versiÃ³n simplificada):")
print("=" * 50)

# La forma mÃ¡s segura es usar el summary directamente
tukey_summary = tukey_results.summary()

# Convertir a DataFrame para mejor visualizaciÃ³n
tukey_data = []
for i in range(len(tukey_results.groupsunique)):
    for j in range(i+1, len(tukey_results.groupsunique)):
        group1 = tukey_results.groupsunique[i]
        group2 = tukey_results.groupsunique[j]
        idx = i * len(tukey_results.groupsunique) + j - (i+1)*(i+2)//2

        tukey_data.append({
            'Modelo 1': group1,
            'Modelo 2': group2,
            'Diferencia': tukey_results.meandiffs[idx],
            'p-ajustado': tukey_results.pvalues[idx],
            'Significativo': tukey_results.reject[idx]
        })

tukey_df = pd.DataFrame(tukey_data)
print(tukey_df.to_string(index=False))

# InterpretaciÃ³n clara
print("\nðŸ” RESUMEN DE COMPARACIONES:")
for _, row in tukey_df.iterrows():
    if row['Significativo']:
        print(f"ðŸŽ¯ {row['Modelo 1']} vs {row['Modelo 2']}: DIFERENCIA SIGNIFICATIVA (p={row['p-ajustado']:.4f})")
    else:
        print(f"âž– {row['Modelo 1']} vs {row['Modelo 2']}: Sin diferencia significativa (p={row['p-ajustado']:.4f})")

"""HIPERPARAMETRIZACION"""

best_models ={}

param_grids ={
  'Stacking':{
      'final_estimator__C':[1, 10],
  },
  'AdaBoost':{
      'learning_rate':[ 0.1, 1.0],
      'n_estimators':[50, 100],
  },
  'KNN':{
      'n_neighbors':[3, 5, 7],
      'weights':['uniform', 'distance'],
  }

}


for model_name in mejores_modelos:
  print(f"\n--- Optimizando {model_name} ---")

  # Definir el modelo base segÃºn el tipo
  if model_name == 'Stacking':
      base_estimators = [
          ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
          ('dt', DecisionTreeClassifier(random_state=42)),
          ('knn', KNeighborsClassifier(n_neighbors=5)),
          ('svm', SVC(probability=True, random_state=42)),
      ]
      meta_estimator = LogisticRegression(random_state=42, max_iter=1000)
      model = StackingClassifier(estimators=base_estimators,
                                final_estimator=meta_estimator, cv=3, passthrough=False,n_jobs=1)
      X_optim = X_train

  elif model_name == 'AdaBoost':
      model = AdaBoostClassifier(random_state=42)
      X_optim = X_train

  elif model_name == 'KNN':
      model = KNeighborsClassifier()
      X_optim = X_train_sacled


  #grid Search
  grid_search = GridSearchCV(estimator=model,
                           param_grid=param_grids[model_name],
                           cv=StratifiedKFold(n_splits=5,shuffle=True,random_state=42),
                           scoring='accuracy',
                           n_jobs=-1,
                           verbose=1,
                           return_train_score=True
                           )
  grid_search.fit(X_optim, y_train)

  best_models[model_name]={
    'best_params':grid_search.best_params_,
    'best_score':grid_search.best_score_,
    'best_estimator':grid_search.best_estimator_,
    'cv_results':pd.DataFrame(grid_search.cv_results_)
}
  print(f"mejores parametros {grid_search.best_params_}")
  print(f"mejor score {grid_search.best_score_}")
  print(f"mejor estimador {grid_search.best_score_ - resultados[model_name].get('Accuracy', 0):.4f}")

"""evaluacion final de los modelos

"""

final_resultados={}

for model_name, model_info in best_models.items():
  print(f"\n--- Evaluando {model_name} ---")

  beter_model = model_info['best_estimator']

  if model_name == 'KNN':
    X_test_optim = X_test_scaled
  else:
    X_test_optim = X_test

  y_pred = beter_model.predict(X_test_optim)
  y_pred_proba = beter_model.predict_proba(X_test_optim)[:, 1]

  #metrics
  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  f1 = f1_score(y_test, y_pred)
  roc_auc = roc_auc_score(y_test, y_pred_proba)


  #matriz de confusion

  cm = confusion_matrix(y_test, y_pred)

  final_resultados[model_name] = {
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1,
    'ROC AUC': roc_auc,
    'confusion_matrix': cm,
    'best_params': model_info['best_params'],

}
  print(f"Accuracy: {accuracy:.4f}")
  print(f"Precision: {precision:.4f}")
  print(f"Recall: {recall:.4f}")
  print(f"F1-Score: {f1:.4f}")
  print(f"ROC AUC: {roc_auc:.4f}")

print(f"\nðŸ“Š Modelos evaluados: {list(final_resultados.keys())}")
  #Comparacion Final
print("\n" + "=" * 50)
print("COMPARACIÃ“N FINAL DE MODELOS OPTIMIZADOS")
print("=" * 50)

final_comparacion = []
for model_name, metrics in final_resultados.items():
    final_comparacion.append({
        'Modelo': model_name,
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-Score': metrics['F1-Score'],
        'ROC AUC': metrics['ROC AUC']
    })

final_df = pd.DataFrame(final_comparacion)
final_df = final_df.sort_values('Accuracy', ascending=False)

print("Ranking final de modelos optimizados:")
print(final_df.to_string(index=False))

# VisualizaciÃ³n de resultados finales
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

#1
models_final = final_df['Modelo']
x_final = np.arange(len(models_final))
width = 0.2

metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ['skyblue', 'lightgreen', 'gold', 'lightcoral']

for i, metric in enumerate(metrics_to_plot):
    axes[0, 0].bar(x_final + i*width, final_df[metric], width, label=metric, alpha=0.8, color=colors[i])

axes[0, 0].set_title('ComparaciÃ³n Final - MÃ©tricas Principales', fontsize=14, fontweight='bold')
axes[0, 0].set_xlabel('Modelo')
axes[0, 0].set_ylabel('Score')
axes[0, 0].set_xticks(x_final + width*1.5)
axes[0, 0].set_xticklabels(models_final, rotation=45)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
axes[0, 0].set_ylim(0, 1)

#2
axes[0, 1].bar(models_final, final_df['ROC AUC'], alpha=0.7, color='purple')
axes[0, 1].set_title('ComparaciÃ³n Final - ROC AUC', fontsize=14, fontweight='bold')
axes[0, 1].set_xlabel('Modelo')
axes[0, 1].set_ylabel('ROC AUC')
axes[0, 1].tick_params(axis='x', rotation=45)
axes[0, 1].grid(True, alpha=0.3)
axes[0, 1].set_ylim(0, 1)
#3
# GrÃ¡fico 3: Mejores parÃ¡metros
axes[1, 0].axis('off')
param_text = "MEJORES PARÃMETROS ENCONTRADOS:\n\n"
for model_name in final_df['Modelo']:
    params = final_resultados[model_name]['best_params']
    param_text += f"ðŸ”§ {model_name}:\n"
    for key, value in params.items():
        param_text += f"   {key}: {value}\n"
    param_text += "\n"

axes[1, 0].text(0.05, 0.95, param_text, transform=axes[1, 0].transAxes,
                fontsize=9, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.7))
#4
axes[1, 1].axis('off')
improvement_text = "RESUMEN DE MEJORAS:\n\n"
for model_name in final_df['Modelo']:
    original_acc = resultados[model_name].get('Accuracy', 0)
    optimized_acc = final_resultados[model_name]['Accuracy']
    improvement = optimized_acc - original_acc

    improvement_text += f"ðŸ“ˆ {model_name}:\n"
    improvement_text += f"   Original: {original_acc:.4f}\n"
    improvement_text += f"   Optimizado: {optimized_acc:.4f}\n"
    improvement_text += f"   Mejora: {improvement:+.4f}\n\n"

axes[1, 1].text(0.05, 0.95, improvement_text, transform=axes[1, 1].transAxes,
                fontsize=9, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))

plt.tight_layout()
plt.show()

"""Seleccion de Modelo Final"""

print("\n" + "=" * 40)
print("4.3.5 SELECCIÃ“N DEL MODELO FINAL")
print("=" * 40)

best_final_model_name = final_df.iloc[0]['Modelo']
best_final_model = best_models[best_final_model_name]['best_estimator']
best_final_metrics = final_resultados[best_final_model_name]

print(f"ðŸ† MODELO FINAL SELECCIONADO: {best_final_model_name}")
print("\nðŸ“Š MÃ‰TRICAS DEL MODELO FINAL:")
print(f"   Accuracy: {best_final_metrics['Accuracy']:.4f}")
print(f"   Precision: {best_final_metrics['Precision']:.4f}")
print(f"   Recall: {best_final_metrics['Recall']:.4f}")
print(f"   F1-Score: {best_final_metrics['F1-Score']:.4f}")
print(f"   ROC AUC: {best_final_metrics['ROC AUC']:.4f}")

print(f"\nðŸ”§ MEJORES PARÃMETROS ENCONTRADOS:")
for key, value in best_final_metrics['best_params'].items():
    print(f"   {key}: {value}")
    # Mostrar matriz de confusiÃ³n del modelo final
print(f"\nðŸ“‹ MATRIZ DE CONFUSIÃ“N DEL MODELO FINAL:")
print(best_final_metrics['confusion_matrix'])
# Guardar el modelo final
import joblib
model_filename = f'mejor_modelo_{best_final_model_name}.pkl'
joblib.dump(best_final_model, model_filename)
print(f"\nðŸ’¾ Modelo guardado como: '{model_filename}'")

print("\n" + "=" * 60)
print("RESUMEN EJECUTIVO - PROCESO DE SELECCIÃ“N COMPLETADO")
print("=" * 60)
print(f"1. Modelos evaluados inicialmente: {len(resultados)}")
print(f"2. Top 3 modelos seleccionados: {mejores_modelos}")
print(f"3. Modelos optimizados exitosamente: {list(best_models.keys())}")
print(f"4. Resultado ANOVA: {'Diferencias significativas' if p_value < 0.05 else 'Sin diferencias significativas'} (p={p_value:.4f})")
print(f"5. Modelo final seleccionado: {best_final_model_name}")
print(f"6. Mejor Accuracy obtenido: {best_final_metrics['Accuracy']:.4f}")
print(f"7. Mejor ROC AUC obtenido: {best_final_metrics['ROC AUC']:.4f}")
print("=" * 60)

# CONCLUSIÃ“N ESTADÃSTICA
print("\n" + "=" * 60)
print("CONCLUSIÃ“N ESTADÃSTICA")
print("=" * 60)
print("Basado en el test de Tukey HSD:")
print("âœ… Los 3 mejores modelos NO presentan diferencias estadÃ­sticamente")
print("   significativas en su performance.")
print("âœ… Esto significa que cualquiera de los 3 modelos podrÃ­a seleccionarse")
print("   como modelo final sin comprometer significativamente el rendimiento.")
print(f"âœ… La selecciÃ³n final se basa en la mejor mÃ©trica de Accuracy: {best_final_model_name}")
print("=" * 60)

# MOSTRAR DETALLE COMPLETO DE TODOS LOS MODELOS
print("\n" + "=" * 60)
print("DETALLE COMPLETO DE RESULTADOS")
print("=" * 60)
for model_name in final_df['Modelo']:
    metrics = final_resultados[model_name]
    print(f"\nðŸ” {model_name}:")
    print(f"   Accuracy: {metrics['Accuracy']:.4f}")
    print(f"   Precision: {metrics['Precision']:.4f}")
    print(f"   Recall: {metrics['Recall']:.4f}")
    print(f"   F1-Score: {metrics['F1-Score']:.4f}")
    print(f"   ROC AUC: {metrics['ROC AUC']:.4f}")