# -*- coding: utf-8 -*-
"""MineriaFinalFase3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RfxiBbatCguU7JgalT7fEcITsFkG012e

# Librerias
"""

import pandas as pd
from ydata_profiling import ProfileReport
import numpy as np

"""# Carga, visualizacion de los datos y Limpieza

## Carga
"""

df = pd.read_csv("heart.csv")
df.head()

"""observamos las dimensiones del dataset"""

df.shape

"""age

sex

chest pain type (4 values)

resting blood pressure

serum cholestoral in mg/dl

fasting blood sugar > 120 mg/dl

resting electrocardiographic results (values 0,1,2)

maximum heart rate achieved

exercise induced angina

oldpeak = ST depression induced by exercise relative to rest

the slope of the peak exercise ST segment

number of major vessels (0-3) colored by flourosopy

thal: 0 = normal; 1 = fixed defect; 2 = reversable defect
"""

df.info()

"""## Visualizacion"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar tu dataset
df = pd.read_csv("heart.csv")  # o el nombre de tu archivo

# Seleccionar solo columnas numéricas continuas
numeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']

plt.figure(figsize=(12,6))
df[numeric_cols].boxplot()
plt.title("Boxplot de variables numéricas")
plt.ylabel("Valores")
plt.show()

for col in numeric_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col], kde=True, bins=20, color='orange')
    plt.title(f"Histograma y densidad de {col}")
    plt.show()

sns.scatterplot(x='age', y='trestbps', data=df)
plt.title("Edad vs Presión en reposo (posibles outliers)")
plt.show()

sns.scatterplot(x='age', y='chol', data=df)
plt.title("Edad vs Colesterol (posibles outliers)")
plt.show()

report = ProfileReport(df, title="My Data Profile", explorative=True)
report.to_notebook_iframe()

"""Vamos a atacar los datos que salieron con alertas

## Limpieza
"""

# Mostrar cantidad de duplicados antes
print("Duplicados antes:", df.duplicated().sum())

# Eliminar duplicados
df = df.drop_duplicates()

# Mostrar resultado
print("Duplicados después:", df.duplicated().sum())
print("Tamaño final del dataset:", df.shape)

"""Dependencias"""

import numpy as np
from scipy.stats import spearmanr
from sklearn.feature_selection import mutual_info_regression
import seaborn as sns
import matplotlib.pyplot as plt

# 1. CORRELACIÓN DE SPEARMAN (detecta relaciones monotónicas)
print("="*60)
print("CORRELACIÓN DE SPEARMAN (relaciones no lineales)")
print("="*60)

features = ['age', 'ca', 'chol', 'cp', 'exang', 'fbs',
                    'oldpeak', 'restecg', 'sex', 'slope',
                     'thal', 'thalach', 'trestbps']
X = df[features]
y = df['target']

spearman_corr = pd.DataFrame()
spearman_corr['Feature'] = features
spearman_corr['Spearman'] = [spearmanr(X[col], y)[0] for col in features]
spearman_corr['Pearson'] = [df[[col, 'target']].corr().iloc[0, 1] for col in features]
spearman_corr['Diferencia'] = abs(spearman_corr['Spearman'] - spearman_corr['Pearson'])

# Ordenar por diferencia (detecta no linealidad)
spearman_corr = spearman_corr.sort_values('Diferencia', ascending=False)
spearman_corr['No_Lineal'] = spearman_corr['Diferencia'].apply(
    lambda x: '⚠️ POSIBLE' if x > 0.1 else '✅ Lineal'
)

print(spearman_corr.to_string(index=False))


# 2. MUTUAL INFORMATION (detecta cualquier dependencia)
print("\n" + "="*60)
print("MUTUAL INFORMATION (cualquier tipo de relación)")
print("="*60)

mi_scores = mutual_info_regression(X, y, random_state=42)
mi_data = pd.DataFrame({
    'Feature': features,
    'MI_Score': mi_scores
}).sort_values('MI_Score', ascending=False)

print(mi_data.to_string(index=False))

from statsmodels.stats.outliers_influence import variance_inflation_factor

features_para_vif = ['age', 'ca', 'chol', 'cp', 'exang', 'fbs',
                    'oldpeak', 'restecg', 'sex', 'slope',
                     'thal', 'thalach', 'trestbps']

X = df[features_para_vif]

# Calcular VIF
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Ordenar por VIF descendente
vif_data = vif_data.sort_values('VIF', ascending=False).reset_index(drop=True)

# Añadir interpretación
vif_data['Status'] = vif_data['VIF'].apply(
    lambda x: '❌ ELIMINAR' if x > 10 else ('⚠️ REVISAR' if x > 5 else '✅ OK')
)

print("\n" + "="*60)
print("ANÁLISIS DE MULTICOLINEALIDAD (VIF)")
print("="*60)
print(vif_data.to_string(index=False))
print("\n" + "="*60)
print("Interpretación:")
print("  VIF < 5:  ✅ Sin problemas de multicolinealidad")
print("  VIF 5-10: ⚠️  Multicolinealidad moderada")
print("  VIF > 10: ❌ Multicolinealidad severa - Considerar eliminar")
print("="*60)

"""### Variables con VIF CRÍTICO (> 10):
Eliminar definitivamente:

trestbps (58.8) - Presión arterial: VIF altísimo + correlación 0.000 con target

chol (26.3) - Colesterol: VIF muy alto + correlación 0.061 con target

fbs - Aunque VIF bajo (1.27), correlación 0.000 con target

slope (10.0) - Borderline, pero oldpeak (VIF 3.07) es mejor predictor

### Decisión crítica sobre estas:

age (39.6): VIF altísimo PERO correlación 0.260 con target (importante
clínicamente)

thalach (42.6): VIF altísimo PERO correlación 0.406 con target (muy predictivo)

thal (17.1): VIF alto PERO correlación 0.522 con target (¡EL MÁS PREDICTIVO!)
"""

# Aplicar limpieza
df_limpio = df.drop(columns=['trestbps', 'chol', 'fbs', 'slope'])

# Verificar
print(f"Variables originales: {df.shape[1]}")
print(f"Variables después de limpieza: {df_limpio.shape[1]}")
print(f"\nVariables finales: {list(df_limpio.columns)}")

df_limpio.head()

df_limpio.shape

"""Outliers"""

# CÓDIGO SUGERIDO PARA AGREGAR:

from scipy import stats

# Método 1: Z-Score (para distribuciones normales)
def remove_outliers_zscore(df, columns, threshold=3):
    df_clean = df.copy()
    for col in columns:
        z_scores = np.abs(stats.zscore(df_clean[col]))
        df_clean = df_clean[z_scores < threshold]
    return df_clean

# Método 2: IQR (Intercuartile Range) - más robusto
def remove_outliers_iqr(df, columns):
    df_clean = df.copy()
    for col in columns:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_clean = df_clean[(df_clean[col] >= lower_bound) &
                            (df_clean[col] <= upper_bound)]
    return df_clean

# Aplicar IQR a variables continuas
numeric_cols = ['age', 'oldpeak', 'thalach']
print(f"Registros antes: {df_limpio.shape[0]}")
df_limpio = remove_outliers_iqr(df_limpio, numeric_cols)
print(f"Registros después: {df_limpio.shape[0]}")

# CÓDIGO SUGERIDO:

# 1. Índice de riesgo cardiovascular
df_limpio['risk_index'] = (
    df_limpio['age'] * 0.3 +
    df_limpio['oldpeak'] * 0.4 +
    df_limpio['ca'] * 0.3
) / 3

# 2. Interacción edad-frecuencia cardíaca
df_limpio['age_thalach_interaction'] = df_limpio['age'] * df_limpio['thalach']

# 3. Categorización de edad
df_limpio['age_group'] = pd.cut(df_limpio['age'],
                                 bins=[0, 45, 60, 100],
                                 labels=['young', 'middle', 'senior'])

# 4. Ratio de variables relacionadas
df_limpio['oldpeak_ca_ratio'] = df_limpio['oldpeak'] / (df_limpio['ca'] + 1)

from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd

# 1. NORMALIZACIÓN de variables numéricas
scaler = StandardScaler()
numeric_features = ['age', 'oldpeak', 'thalach']
df_limpio[numeric_features] = scaler.fit_transform(df_limpio[numeric_features])

# 2. CODIFICACIÓN de variables categóricas
# One-Hot Encoding para variables nominales (ya numéricas después del paso)
df_limpio = pd.get_dummies(
    df_limpio,
    columns=['cp', 'restecg', 'thal', 'age_group'],  #  añadimos age_group
    drop_first=True,
    dtype=int
)

# 3. Asegurar que todas las columnas binarias sean enteros (0 o 1)
df_limpio = df_limpio.astype({
    col: 'int' for col in df_limpio.columns
    if df_limpio[col].dropna().isin([0, 1]).all()
})

# 4. Verificar resultado
print("\nVariables después de transformaciones:")
print(df_limpio.dtypes)
print(f"\nDimensiones finales: {df_limpio.shape}")

df_limpio.info()

df_limpio.shape

df_limpio.describe()

# Exportar df limpio a CSV
df_limpio.to_csv('df_limpio_modelo.csv', index=False)